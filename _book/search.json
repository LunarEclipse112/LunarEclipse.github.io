[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Hi! welcome, portfolio. collection different skills posses way show can data analysis context. page contains different group skills stands alone presentation. Also good know course, ongoing process.","code":""},{"path":"index.html","id":"about-me","chapter":"1 Welcome!","heading":"1.1 About Me","text":"student University applied sciences Utrecht. great passion biology, molecular life\nsciences data analysis. work environment can implement interests dream job \n. specialization biomolecular research data analysis knowledge can bridge\nlab analysis data scientist. transgender nonbinary individual, means go \n/(die/diens hen/hun). Usually isn’t something place much importance , just live \nlife like anyone else might implications future regarding sudden surgeries. case\noccur feel like future employer internship provider aware beforehand.","code":""},{"path":"example-analysis-c.-elegans-experiment.html","id":"example-analysis-c.-elegans-experiment","chapter":"2 Example analysis C. Elegans experiment","heading":"2 Example analysis C. Elegans experiment","text":"First look case analysis C. elegans plate experiment. data experiment kindly supplied J. Louter (INT/ILC). exposure various different compounds certain incubation time C. elegans nematode offspring counted determine toxicity compounds. data supplied excel file Data Link.looking data number things popped . compound unit values aren’t . values controls listed ‘pct’, plot graph see comparison nematode offspring treated control groups. wil allow us make guess efficacy compounds question. S-medium Ethanol controls study. Ethanol concentration 1.5 pct positive control, Medium-S negative control concentration 0.0 pct lastly Ethanol concentration 0.5 pct control vehicle . better illustrate position values graph set amount jitter added, 0.1 width none height, just pull apart points better visualization. y-axis values remain thus unchanged seeing concentration values within groups disturb data.Looking graph can estimate might correlation less offspring counted trated colonies C. Elegans higher concentrations Naphtalene, 2,6-diisopropylnaphthalene, decane Ethanol. continue test assumption data normalized plotted using min-max scaling. allow us better look different compounds compared .Based last graph able say might correlation higher dosage different compounds (controls, except positive control) reduced offspring different C. elegans populations counted.","code":""},{"path":"open-peer-review.html","id":"open-peer-review","chapter":"3 Open Peer Review","heading":"3 Open Peer Review","text":"context reproducibility wise ensure quality products kept high analyzing reviewing peoples work. due time recognizing bad transparency become easier ones work improve.","code":""},{"path":"open-peer-review.html","id":"teaching-anxiety-stress-and-resilience-during-the-covid-19-pandemic","chapter":"3 Open Peer Review","heading":"3.0.1 Teaching anxiety, stress and resilience during the COVID-19 pandemic","text":"first article selected reviewing teaching anxiety, stress resilience COVID-19 pandemic (Delgado-Gallegos et al. 2021).article looks psychological changes among teachers changed school attending curriculum virtual classroom model following restrictions face face meeting due pandemic quarantine.start scoring article based rubric determines reproducibility allow quick overview.Reviewing rubric shows us article professional overall. data supplied allows later testing reproducibility. Although actually check reproducibility need write code run analysis check variances results, can’t fully conclude article question fully reproducible. code supplied article beautiful example reproducible research.","code":""},{"path":"open-peer-review.html","id":"monitoring-trends-and-differences-in-covid-19-case-fatality-rates-using-decomposition-methods-contributions-of-age-structure-and-age-specific-fatality","chapter":"3 Open Peer Review","heading":"3.0.2 Monitoring trends and differences in COVID-19 case-fatality rates using decomposition methods: Contributions of age structure and age-specific fatality","text":"Now going look code article qualify reproducible research. next paper Monitoring trends differences COVID-19 case-fatality rates using decomposition methods: Contributions age structure age-specific fatality (Dudel et al., n.d.).First lets score article question using rubric :sets article apart previous one possibility look code used produce results raw data. raw data links exact code supplied article thus allow easy reproduction research done. important note however article doesn’t score perfectly rubric seen .code found R file name distributed based function code different scripts. scripts easy read clear function. Every section well defined borders short clear comments purpose chunk . rate code 4/5 first inspection. reason don’t give full score personal preference. prefer different scripts simply one script easy run instead reloading data change every time, ’s just .first script (00_functions.R) defines custom functions: one summarises data another function compare case fatality rates age distribution two groups.\nsecond code file (01_input.R) loads raw data, adds column (see mutate() function used), filters transfers new .csv file later usage.\nthird script (02_analysis.R) modifies data bit runs basic analysis .won’t go detail scripts analysis research purpose open peer review fragment. however try reproduce small portion following segment. changes code recorded comments.looking code plotting first figure can conclude following. code supplied beautifully made small change file location runs perfectly. one means reproducible research data.\nscore code research paper 5 5 ease use exactly supposed without noise .","code":"\n### Packages ##################################################################\n\n  library(tidyverse)\n  library(ggrepel)\n  library(scales)\n\n### Load data #################################################################\n\n  # Load data\n  db_gh <- read_csv(here::here(\"./data/inputdata.csv\"))\n  ##### Small change to the location of the file so i can run it properly\n\n### Aggregate data ############################################################\n\n  # Filter date\n  db_gh$Date <- as.Date(db_gh$Date,\"%d.%m.%y\")\n  db_gh2 <- db_gh %>% filter(Date<=as.Date(\"30.06.2020\",\"%d.%m.%y\"))\n  \n  # Set New York as \"country\" (easier handling)\n  db_gh2$Country[db_gh2$Country==\"USA\" & db_gh2$Region == \"NYC\"] <- \"NYC\"\n  \n  # Sum data over age groups\n  db_gh2 <- db_gh2 %>% \n    filter(!Country %in% c(\"China\",\"USA\",\"South Korea\") & Sex == \"b\") %>% \n    group_by(Country, Code,Date) %>% \n    summarise(Cases = sum(Cases),\n              Deaths = sum(Deaths))\n\n  # Exclude bolletino \n  db_gh2 <- db_gh2 %>%\n    filter(str_sub(Code, 1, 5) != \"ITbol\")\n  \n  # Sort by date\n  db_gh2 <- db_gh2 %>% group_by(Country) %>% arrange(Date)\n  \n  # Smooth reporting issues cases\n  for(country in unique(db_gh2$Country)) {\n    \n    days <- db_gh2$Date[db_gh2$Country==country]\n    \n    for(day in 2:length(days)) {\n      current <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]]\n      previous <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day-1]]\n      \n      if(current<previous) db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous\n      \n    }\n    \n  }\n\n  # Smooth reporting issues deaths\n  for(country in unique(db_gh2$Country)) {\n    \n    days <- db_gh2$Date[db_gh2$Country==country]\n    \n    for(day in 2:length(days)) {\n      current <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]]\n      previous <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day-1]]\n      \n      if(current<previous) db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous\n      \n    }\n    \n  }\n  \n### Plot settings #############################################################\n\n  # Set colors\n  col_country <- c(\"Germany\" = \"black\",\n                   \"Italy\" = \"#2ca25f\",\n                   \"NYC\"=\"#f0027f\",\n                   \"Spain\"=\"#beaed4\",\n                   \"South Korea\"=\"#fdc086\")#,\n                   #\"USA\"=\"#386cb0\")\n  \n  cols <- c(\"black\",\n            \"#2ca25f\",\n            \"#f0027f\",\n            \"#beaed4\",\n            \"#fdc086\")#,\n            #\"#386cb0\")\n  \n  \n  # Axis\n  labs <- db_gh2 %>%\n    group_by(Country) %>% \n    filter(Cases == max(Cases)) %>% \n    mutate(Cases = Cases + 3000)\n  \n  # Including all reports\n  tx <- 6\n  lim_x <- 240000\n\n### Plot ######################################################################\n\n  db_gh2 %>% \n    ggplot(aes(Cases, Deaths, col = Country))+\n    geom_line(size = 1, alpha = .9)+\n    scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+\n    scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"text\", label = \"2% CFR\", x = lim_x + 1000, y = lim_x * .02,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"5% CFR\", x = lim_x + 1000, y = lim_x * .05,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"10% CFR\", x = lim_x + 1000, y = lim_x * .10,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"15% CFR\", x = lim_x + 1000, y = lim_x * .15,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    scale_colour_manual(values = cols)+\n    geom_text(data = labs, aes(Cases, Deaths, label = Country),\n              size = tx * .35, hjust = 0, fontface = \"bold\") +\n    theme_classic()+\n    labs(x = \"Cases\", \n         y = \"Deaths\")+\n    theme(\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.margin = margin(5,5,5,5,\"mm\"),\n      axis.text.x = element_text(size = tx),\n      axis.text.y = element_text(size = tx),\n      axis.title.x = element_text(size = tx + 1),\n      axis.title.y = element_text(size = tx + 1)\n    )"},{"path":"guerrilla-analytics.html","id":"guerrilla-analytics","chapter":"4 Guerrilla Analytics","heading":"4 Guerrilla Analytics","text":"working together groups using github alone ones silico interface great importance keep consistent organized structure easily find data code. one hand crucial avoid confusion version usage (v4_file instead latestlateslatest_file) established files data named README.txt file explanation METADATA file. hand makes writing code edit, analyse plot data much easier path data files fixed. Guerrilla analytics reproducibility allows anyone quickly understand files data contain helps analysts write easier code deepening file locations.short example set type shown . organized one courses guerrilla analytics format better explain structure. easy work much enjoy looking well organized project.Although course organized properly allot missing items like missing code chunks due running code changes data Rmarkdown instead individual scripts. Note also consists 3 projects, just one. means depth comparison single project, said , depth creates chaos. run trough linux server. next example can see structure project, portfolio project (bookdown project portfolio witch linked trough website!). organized structure format pc. allows easier access compared server designed learn run learning institution.\npath shown longer necessary working server github project.Next looking README Portfolio project. Listed content README file show information files project. can also just look README linked Github link. one little bit easier read.","code":"# Using this command we can get the structure of our file organisation \nfs::dir_tree(here::here(\"daur2/\"))\n\n/home/1686589/daur2/\n├── rna_seq_ipsc\n│   ├── README.txt\n│   ├── code\n│   │   └── Input.R\n│   ├── data\n│   ├── rnaseq_ipsc.Rmd\n│   └── rnaseq_ipsc.html\n├── rnaseq_airway\n│   ├── README.txt\n│   ├── code\n│   │   ├── input.R\n│   │   └── namen.txt\n│   ├── data\n│   │   ├── airway_sampledata.csv\n│   │   ├── bam\n│   │   │   ├── SRR1039508.bam\n│   │   │   ├── SRR1039508.bam.indel.vcf\n│   │   │   ├── SRR1039508.bam.summary\n│   │   │   ├── SRR1039509.bam\n│   │   │   ├── SRR1039509.bam.indel.vcf\n│   │   │   ├── SRR1039509.bam.summary\n│   │   │   ├── SRR1039512.bam\n│   │   │   ├── SRR1039512.bam.indel.vcf\n│   │   │   ├── SRR1039512.bam.summary\n│   │   │   ├── SRR1039513.bam\n│   │   │   ├── SRR1039513.bam.indel.vcf\n│   │   │   ├── SRR1039513.bam.summary\n│   │   │   ├── SRR1039516.bam\n│   │   │   ├── SRR1039516.bam.indel.vcf\n│   │   │   ├── SRR1039516.bam.summary\n│   │   │   ├── SRR1039517.bam\n│   │   │   ├── SRR1039517.bam.indel.vcf\n│   │   │   ├── SRR1039517.bam.summary\n│   │   │   ├── SRR1039520.bam\n│   │   │   ├── SRR1039520.bam.indel.vcf\n│   │   │   ├── SRR1039520.bam.summary\n│   │   │   ├── SRR1039521.bam\n│   │   │   ├── SRR1039521.bam.indel.vcf\n│   │   │   ├── SRR1039521.bam.summary\n│   │   │   └── alignment_statistics.rds\n│   │   ├── counts\n│   │   │   └── read_counts.rds\n│   │   ├── fastq\n│   │   │   ├── SRR1039508_1.fastq.gz\n│   │   │   ├── SRR1039508_2.fastq.gz\n│   │   │   ├── SRR1039509_1.fastq.gz\n│   │   │   ├── SRR1039509_2.fastq.gz\n│   │   │   ├── SRR1039512_1.fastq.gz\n│   │   │   ├── SRR1039512_2.fastq.gz\n│   │   │   ├── SRR1039513.fastq.gz\n│   │   │   ├── SRR1039513_1.fastq.gz\n│   │   │   ├── SRR1039513_2.fastq.gz\n│   │   │   ├── SRR1039516.fastq.gz\n│   │   │   ├── SRR1039516_1.fastq.gz\n│   │   │   ├── SRR1039516_2.fastq.gz\n│   │   │   ├── SRR1039517_1.fastq.gz\n│   │   │   ├── SRR1039517_2.fastq.gz\n│   │   │   ├── SRR1039520.fastq.gz\n│   │   │   ├── SRR1039520_1.fastq.gz\n│   │   │   ├── SRR1039520_2.fastq.gz\n│   │   │   ├── SRR1039521.fastq.gz\n│   │   │   ├── SRR1039521_1.fastq.gz\n│   │   │   ├── SRR1039521_2.fastq.gz\n│   │   │   ├── fastq_dump_downloader.sh\n│   │   │   ├── fastqc.sh\n│   │   │   └── refgenome_downloader.sh\n│   │   ├── fastqc_output\n│   │   │   ├── SRR1039508_1_fastqc.html\n│   │   │   ├── SRR1039508_1_fastqc.zip\n│   │   │   ├── SRR1039508_2_fastqc.html\n│   │   │   ├── SRR1039508_2_fastqc.zip\n│   │   │   ├── SRR1039509_1_fastqc.html\n│   │   │   ├── SRR1039509_1_fastqc.zip\n│   │   │   ├── SRR1039509_2_fastqc.html\n│   │   │   ├── SRR1039509_2_fastqc.zip\n│   │   │   ├── SRR1039512_1_fastqc.html\n│   │   │   ├── SRR1039512_1_fastqc.zip\n│   │   │   ├── SRR1039512_2_fastqc.html\n│   │   │   ├── SRR1039512_2_fastqc.zip\n│   │   │   ├── SRR1039513_1_fastqc.html\n│   │   │   ├── SRR1039513_1_fastqc.zip\n│   │   │   ├── SRR1039513_2_fastqc.html\n│   │   │   ├── SRR1039513_2_fastqc.zip\n│   │   │   ├── SRR1039513_fastqc.html\n│   │   │   ├── SRR1039513_fastqc.zip\n│   │   │   ├── SRR1039516_1_fastqc.html\n│   │   │   ├── SRR1039516_1_fastqc.zip\n│   │   │   ├── SRR1039516_2_fastqc.html\n│   │   │   ├── SRR1039516_2_fastqc.zip\n│   │   │   ├── SRR1039516_fastqc.html\n│   │   │   ├── SRR1039516_fastqc.zip\n│   │   │   ├── SRR1039517_1_fastqc.html\n│   │   │   ├── SRR1039517_1_fastqc.zip\n│   │   │   ├── SRR1039517_2_fastqc.html\n│   │   │   ├── SRR1039517_2_fastqc.zip\n│   │   │   ├── SRR1039520_1_fastqc.html\n│   │   │   ├── SRR1039520_1_fastqc.zip\n│   │   │   ├── SRR1039520_2_fastqc.html\n│   │   │   ├── SRR1039520_2_fastqc.zip\n│   │   │   ├── SRR1039520_fastqc.html\n│   │   │   ├── SRR1039520_fastqc.zip\n│   │   │   ├── SRR1039521_1_fastqc.html\n│   │   │   ├── SRR1039521_1_fastqc.zip\n│   │   │   ├── SRR1039521_2_fastqc.html\n│   │   │   ├── SRR1039521_2_fastqc.zip\n│   │   │   ├── SRR1039521_fastqc.html\n│   │   │   └── SRR1039521_fastqc.zip\n│   │   ├── hg38_genome\n│   │   │   └── GRCh38.primary_assembly.genome.fa\n│   │   └── hg38_index\n│   │       ├── hg38_index.00.b.array\n│   │       ├── hg38_index.00.b.tab\n│   │       ├── hg38_index.files\n│   │       ├── hg38_index.log\n│   │       ├── hg38_index.reads\n│   │       ├── subread-index-sam-186134-meVx1N\n│   │       └── subread-index-sam-193622-AYN0aX\n│   ├── rna_seq.Rmd\n│   └── rna_seq.html\n└── rnaseq_onecut\n    ├── README.txt\n    ├── Rmarkdown\n    │   ├── rnaseq_eind.Rmd\n    │   ├── rnaseq_eind.html\n    │   ├── rnaseq_eind.log\n    │   ├── rnaseq_eind.tex\n    │   ├── v1_Eindopdracht.Rmd\n    │   └── v2_Eindopdracht.Rmd\n    ├── code\n    ├── data\n    └── output\n        ├── 6_volcano_plot-1.png\n        ├── perbase_SRR7866699.png\n        └── perseq_SRR7866699.pngfs::dir_tree(here::here(\"\"))\n\nC:/Users/31642/Documents/Study/Vakken/Bioinformatica/DSBF2/dsbf2_workflows_portfolio/Portfolio\n+-- bibliography_portfolio\n|   +-- bibliography_portfolio.bib\n|   \\-- files\n|       +-- 11\n|       |   \\-- Sathyanesan and Gallo - 2019 - Cerebellar contribution to locomotor behavior A n.pdf\n|       +-- 13\n|       |   \\-- Dudel et al. - Monitoring trends and differences in COVID-19 case.pdf\n|       +-- 15\n|       |   \\-- Delgado-Gallegos et al. - 2021 - Teaching Anxiety, Stress and Resilience During the.pdf\n|       +-- 18\n|       |   \\-- www.noldus.com.html\n|       +-- 3\n|       |   \\-- Vinueza Veloz et al. - 2012 - The effect of an mGluR5 inhibitor on procedural me.pdf\n|       +-- 5\n|       |   \\-- Schonewille et al. - 2011 - Reevaluating the Role of LTD in Cerebellar Motor L.pdf\n|       +-- 7\n|       |   \\-- Vinueza Veloz et al. - 2015 - Cerebellar control of gait and interlimb coordinat.pdf\n|       \\-- 9\n|           \\-- Sathyanesan et al. - 2018 - Neonatal brain injury causes cerebellar learning d.pdf\n+-- code\n|   +-- 00_functions.R\n|   +-- 01_input.R\n|   +-- 02_analysis.R\n|   +-- 03_excess.R\n|   +-- Fig_1.R\n|   +-- guerrilla_tactics.R\n|   +-- pdftopng.R\n|   \\-- Tests.R\n+-- data\n|   +-- baseline_excess_pclm_5.csv\n|   +-- data_cv.R\n|   +-- dengue_data.csv\n|   +-- dengue_tidy.csv\n|   +-- dengue_tidy.rds\n|   +-- Dudel_et_al_CFR_Decomposition.xlsx\n|   +-- Excel_spreadsheets_decomposition.xlsx\n|   +-- flu_data.csv\n|   +-- flu_tidy.csv\n|   +-- flu_tidy.rds\n|   +-- gapdat_tidy.csv\n|   +-- gapdat_tidy.rds\n|   \\-- inputdata.csv\n+-- output\n|   +-- AppendixTab1.xlsx\n|   +-- AppendixTab2.xlsx\n|   +-- AppendixTab3.xlsx\n|   +-- AppendixTab4.xlsx\n|   +-- AppendixTab5.xlsx\n|   +-- AppendixTab6.xlsx\n|   +-- Fig_1.jpg\n|   +-- Table2.xlsx\n|   +-- Table3.xlsx\n|   \\-- thumbnail_IMG_7494.jpg\n+-- Portfolio.Rproj\n+-- README.txt\n\\-- Rmarkdowns\n    +-- awesome-cv.cls\n    +-- fonts\n    |   +-- FontAwesome.ttf\n    |   +-- Roboto-Bold.ttf\n    |   +-- Roboto-BoldItalic.ttf\n    |   +-- Roboto-Italic.ttf\n    |   +-- Roboto-Light.ttf\n    |   +-- Roboto-LightItalic.ttf\n    |   +-- Roboto-Medium.ttf\n    |   +-- Roboto-MediumItalic.ttf\n    |   +-- Roboto-Regular.ttf\n    |   +-- Roboto-Thin.ttf\n    |   \\-- Roboto-ThinItalic.ttf\n    +-- portfolio_opdracht1.html\n    +-- portfolio_opdracht1_1.html\n    +-- portfolio_opdracht1_1.Rmd\n    +-- portfolio_opdracht1_2.html\n    +-- portfolio_opdracht1_2.Rmd\n    +-- portfolio_opdracht2.html\n    +-- Portfolio_opdracht2.Rmd\n    +-- Portfolio_opdracht3.html\n    +-- Portfolio_opdracht3_1.pdf\n    +-- Portfolio_opdracht3_1.Rmd\n    +-- Portfolio_opdracht3_2.html\n    +-- Portfolio_opdracht3_2.Rmd\n    +-- Portfolio_opdracht5.html\n    +-- Portfolio_opdracht5.Rmd\n    +-- Portfolio_opdracht7.html\n    \\-- Portfolio_opdracht7.Rmd\nread_file(here::here(\"README.md\"))\n#> [1] \"# lunareclipse\\r\\n\\r\\nThis repository contains my portfolio. It is meant to showcase my skills in data analysis. My goal is to be able to present to a future employer what it is I can do and such this portfolio will change greatly over time. As of now I am very much in the beginning of my journey and looking for an internship to improve my skills in a certain direction. For me my interest lies especially in next generation sequencing. I have worked on some NGS projects trough my university and I very much like how it goes hand in hand with understanding the biology of organisms to better interpret the results. \\r\\n\\r\\nMaking this portfolio has been a great learning experience and I will be keeping this update regularly. I have plans to include a RNA-seq analysis, Metagenomics pipeline and to keep working on the scRNA-seq skills as I find it very interesting. I would have liked to have one pipeline finished by now (12-06-2022) for the SnapATAC2 package but sadly I haven't figured out how to install and run the package properly on my device so that stands at the front of my to do list. Also I would like to improve on the skills I already posses and will make the pages in this portfolio go more in depth on the different basic skills to better showcase what I can do.\\r\\n\\r\\nI hope you will enjoy reading this and thank you for your time!\\r\\n\\r\\nIn this project most of the coding is done in R, with a little bit of Bash, CSS, YML, SQL and HTML on the side.\\r\\n\\r\\nWebsite link [here](https://lunareclipse.netlify.app/index.html)\""},{"path":"looking-ahead.html","id":"looking-ahead","chapter":"5 Looking ahead","heading":"5 Looking ahead","text":"","code":""},{"path":"looking-ahead.html","id":"single-cell-ngs","chapter":"5 Looking ahead","heading":"5.1 Single Cell NGS","text":"demonstrate ability learn new skills continue develop try learn much can four days single cell next generation sequencing.chose skill NGS something find interesting NGS single cell sounds awesome.","code":""},{"path":"looking-ahead.html","id":"plan","chapter":"5 Looking ahead","heading":"5.1.1 Plan","text":"First small overview want structure 4 days make sure best make use time. end can hopefully run analysis, part one isn’t complicated. end need read search usable data used research try mimic done. Hopefully future allow better develop skills needed run analysis Single Cell NGS data.","code":""},{"path":"looking-ahead.html","id":"next-generation-sequencing","chapter":"5 Looking ahead","heading":"5.1.2 Next generation sequencing","text":"Next generation sequencing (NGS) massively parallel sequencing technology allows fast precise sequencing DNA cDNA. done parallel sequencing high number DNA (cDNA) fragments provides high depth much insight genomes organisms can used explore gen expression single organism. bioinformatic analysis data acquired can put together form complete genome give insight cellular activity organism (Behjati Tarpey (2013)). technology brought many different advances science now looked implemented diagnostics field noninvasive tool diagnose infectious diseases immunocompromised hosts (Jose F. Camargo 1, Asim . Ahmed 2, Martin S. Lindner2, Michele . Morris1, Shweta Anjan1, Anthony D. Anderson 3, Clara E. Prado4, Sudeb C. Dalai2, Octavio V. Martinez4, Krishna V. Komanduri (n.d.)).Although technology impressive useful wide range applications (Behjati Tarpey (2013)) significant drawback. picture can formed looking RNA expression tissue type specific cells ’s always clouded due interference different cells “sample”. design NGS utilizes large amount cDNA fragments need analysed silico\nalign fragments reference genome gain information genes expressed. problem arises one wants accurately look much certain gene expressed. Due differences cellular expression cells accuracy determination gene expression always limited. Two cells type might producing different quantities RNA used measured NGS analysis (cDNA) differences can’t retraced run utilizes cDNA fragments multiple cells. solution problem (among others) run differential gene expression analysis (DGE). DGA analysis provides information statistical difference expressed genes based probability value. reliable method find Log2 fold changes quite accurately calculate answer much question problem. Another solution single cell next generation sequencing also provides answers problem.","code":""},{"path":"looking-ahead.html","id":"single-cell-next-generation-sequencing","chapter":"5 Looking ahead","heading":"5.1.3 Single cell next generation sequencing","text":"Single cell transcriptomics used characterize cell states (Stuart et al. (2019)). Ranging gene expression spatial transcriptomics chromatine accesibility immunophenotyping scRNA-seq technology broad spectrum applications, strengths weaknesses. measure specific aspect cellular identity posing clear need use information acquired one analysis strengthen conclusion another.","code":""},{"path":"looking-ahead.html","id":"scatac-seq","chapter":"5 Looking ahead","heading":"5.1.4 scATAC-seq","text":"Single cell NGS used research better understand developmental programs encoded linear genome sequence (Fang et al. (2021)). Single cell NGS can also used identify cis-elements organisms specific cell types. Cell type established due regulation spatiotemporal gene expression programs (ENCODE Project Consortium (2012)). interact sequence specific transcription factors cis-regulatory sequences cell-type specific manner. Due sensitivity nucleases transposases active different methods developed find de cis-elements organisms specific cell types. Chromatine accesability methods example scTHIS-seq (single-cell transposome hypersensitive site sequencing) scATAC-seq. scATA-seq single-cell ATAC-seq analysis allows profiling chromatine accessibility hundreds thousands cells single experiment (Fang et al. (2021)).Current methods often require use linear dimensionality reduction cell matrix thousands dimensions. Scaling millions cells huge challenge. addition scATAC-seq datasets posses level sensitivity scRNA-seq. overcome limitations single nucleus analysis pipeline ATAC-seq-SnapATAC proposed trying .","code":""},{"path":"looking-ahead.html","id":"snapatac","chapter":"5 Looking ahead","heading":"5.1.5 SnapATAC","text":"posed now trying package SnapATAC provided dataset 10X (author). package runs python take time expected 4 days ’s big deal.","code":""},{"path":"looking-ahead.html","id":"python-in-r","chapter":"5 Looking ahead","heading":"5.1.6 Python in R","text":"use python code chunks Rmarkdown downloaded reticulate package. chunk running python.Now download SnapATAC2 package need pip maybe Anaconda. yet sure better option trying get Anaconda environment can maybe download package run trough Rstudio. also trying get install pip, easier think.\nSadly using pip worked. can get pip install packages just fine SnapAtac2 doesn’t want install. checked issues github page package found rust anndate package able help downloaded still nothing. also read snapatac2 hadn’t tested windows older message maybe ’s issue. Lets proceed supposed work see can get .\npip failing left Anaconda. Let’s try download simple package first see works properly. check anaconda works installed numpy package. download successful downloading snapatac2 . ’s time look different package, prefer find one R, comfortable working R packages limited time.","code":"\nprint(\"Hello World\")\n#> Hello World"},{"path":"looking-ahead.html","id":"cellwalkr","chapter":"5 Looking ahead","heading":"5.1.7 CellWalkR","text":"next package found CellWalkR. CellWalkR package combine scATAC-seq data labels epigenetic data. using data linked vignette pre processed ArchR. also install ArchR future use.data linked article (Ziffra et al. (2021)). won’t go depth article just now. now just going collect data follow vignette tutorial.","code":"\n# Package requires devtools and BiocManager\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\n# Then install Archr\ndevtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n# Lastly install the dependencies\nlibrary(ArchR)\nArchR::installExtraPackages()\n### Load in the Data ###########################################################\n  library(ArchR)\n  downloadSampleData(\"vignette_data\")\n#> [1] \"vignette_data\"\n#> [1] \"vignette_data\"\n  ATACMat <- Matrix::readMM(\"vignette_data/SamplePeakMat.mtx\")\n  peaks <- as(data.table::fread(\"vignette_data/SamplePeaks.txt\", header = FALSE)$V1, \"GRanges\")\n\n### Define Label Nodes #########################################################\n\n  pathToLabels <- system.file(\"extdata\", \"SampleMarkers1.txt\", package = \"CellWalkR\")\n  labelGenes <- data.table::fread(pathToLabels)\n\n# Check the labeling data. It has to have at least two columns\n  head(labelGenes)\n#>    entrez  cluster   avg_diff\n#> 1:  10299 RG-early -1.2297890\n#> 2:   6167 RG-early  0.2546596\n#> 3:  11168 RG-early  0.2570446\n#> 4:   8760 RG-early -0.2578798\n#> 5:   8503 RG-early  0.2613031\n#> 6:  10208 RG-early  0.2618003\n  \n### Building a Network #########################################################\n  \n# Next we compute cell-to-cell similarity in order to build edges in the cell-to-cell proportion of the graph\n  \n  cellEdges <- computeCellSim(ATACMat)\n  \n# Check cellEdges \n  cellEdges[1:5,1:5]\n#> 5 x 5 sparse Matrix of class \"dgCMatrix\"\n#>                                                            \n#> [1,] 1.00000000 0.11338151 0.15001705 0.09244314 0.09813385\n#> [2,] 0.11338151 1.00000000 0.14950372 0.08130564 0.09035017\n#> [3,] 0.15001705 0.14950372 1.00000000 0.08960442 0.11350499\n#> [4,] 0.09244314 0.08130564 0.08960442 1.00000000 0.06237177\n#> [5,] 0.09813385 0.09035017 0.11350499 0.06237177 1.00000000\n  \n### Computing Label-Cell Edges #################################################\n  \n# Now we need to define which genomic regions correspond to which genes. We’ll use hg38 with Entrez identifiers in this example, using full gene bodies\n  regions <- getRegions(geneBody = TRUE, genome = \"hg38\", names = \"Entrez\")\n# Check it\n  head(regions)\n#> GRanges object with 6 ranges and 1 metadata column:\n#>             seqnames            ranges strand |     gene_id\n#>                <Rle>         <IRanges>  <Rle> | <character>\n#>           1    chr19 58362552-58364751      - |           1\n#>          10     chr8 18389282-18391481      + |          10\n#>         100    chr20 44652034-44654233      - |         100\n#>        1000    chr18 28176931-28179130      - |        1000\n#>   100009613    chr11 70075234-70077433      - |   100009613\n#>   100009667    chr10 68010663-68012862      - |   100009667\n#>   -------\n#>   seqinfo: 595 sequences (1 circular) from hg38 genome\n# Now we map between this data and the peaks\n  ATACGenePeak <- mapPeaksToGenes(labelGenes, ATACMat, peaks, regions)\n  labelEdges <- computeLabelEdges(labelGenes, ATACMat, ATACGenePeak)\n# Check it\n  head(labelEdges)\n#>      RG-early        oRG        tRG         vRG    RG-div1\n#> [1,]        0 0.04201235 0.04275655 0.019920365 0.05198877\n#> [2,]        0 0.03108851 0.02250872 0.008458454 0.03121237\n#> [3,]        0 0.02902324 0.02383554 0.008164892 0.03279098\n#> [4,]        0 0.03035606 0.01968420 0.009663494 0.03510777\n#> [5,]        0 0.02334971 0.02856838 0.005171970 0.03755360\n#> [6,]        0 0.03310487 0.02331299 0.008916823 0.03410891\n#>         RG-div2\n#> [1,] 0.04404968\n#> [2,] 0.02986432\n#> [3,] 0.03135008\n#> [4,] 0.02916076\n#> [5,] 0.03137661\n#> [6,] 0.03400801\n  \n### Tuning Label Edges #########################################################\n  \n# We now have cell-to-cell edges and label-to-cell edges, we don’t know how to correctly weight the two relative to each other. The tuneEdgeWeights method will run CellWalker across a range of possible parameters and compute cell homogeneity for each\n  labelEdgesList <- list(labelEdges)\n  \n  edgeWeights <- tuneEdgeWeights(cellEdges, \n                              labelEdgesList, \n                              labelEdgeOpts = 10^seq(1,7,1), \n                              sampleDepth = 1000)\n# Check it\n  head(edgeWeights[order(edgeWeights$cellHomogeneity, decreasing = TRUE),])\n#>    Var1 cellHomogeneity\n#> 6 1e+06     -0.02088516\n#> 7 1e+07     -0.03962624\n#> 4 1e+04     -0.04596096\n#> 5 1e+05     -0.04841411\n#> 3 1e+03     -0.05248577\n#> 2 1e+02     -0.27341516\n\n### Making a cellWalk Object ###################################################\n\n# Now we generate a cellWalk object with the above tuned edge weight parameter\n  cellWalk <- walkCells(cellEdges, \n                     labelEdgesList, \n                     labelEdgeWeights = 1e+07)\n  \n### Adding Filters #############################################################\n\n# We may have some bulk epigenetic data that can help filter down which peaks are relevant to our analysis. We can tune weights on each filter to determine how significant it is to our data. For our example we have H3K4me3 data which indicates active promoters. Thus we apply this filter permissively (setting filterOut=FALSE) and at the whole gene level (filterGene=TRUE) rather than just to overlapping peaks. We can make a new cellWalk object using this filter:\n  \n  # labelEdges <- computeLabelEdges(labelGenes, \n  #                              ATACMat, \n  #                              ATACGenePeak,\n  #                              filters = filters, \n  #                              filterWeights = c(1),\n  #                              filterOut = c(FALSE),\n  #                              filterGene = c(TRUE),\n  #                              regions = regions)\n  # labelEdgesList <- list(labelEdges)\n  # cellWalk <- walkCells(cellEdges, \n  #                    labelEdgesList, \n  #                    labelEdgeWeights = 1e+07)\n\n### Downstream Analysis ########################################################\n\n# Once we have a cellWalk object we can use it for downstream analysis\n  \n# The label threshold determines the minimum influence score a cell must get to be considered labeled\n  cellWalk <- findUncertainLabels(cellWalk, labelThreshold = 0, plot = TRUE)\n\n# We can also directly examine label similarity by considering label-to-label influence\n  cellWalk <- clusterLabels(cellWalk,  plot = TRUE)#> NULL\n  \n  cellWalk <- plotCells(cellWalk, labelThreshold = 0, seed = 1)\n    \n# It is also possible to plot how strongly a single label influences each cell in the embedding\n  cellWalk <- plotCells(cellWalk, cellTypes = c(\"RG-div2\"), seed = 1)\n  \n# Furthermore, to analyze rare cell types, it can be helpful to only plot a subset of of all labels\n  cellWalk <- plotCells(cellWalk, cellTypes = c(\"RG-early\",\"tRG\",\"vRG\"), labelThreshold = 0, seed = 1)"},{"path":"looking-ahead.html","id":"whats-next","chapter":"5 Looking ahead","heading":"5.1.8 What’s Next","text":"next steps like keep trying get SnapAtac work also try Cellwalker package another research actually run analysis. now time done continue work subject find Interesting now page ’s portfolio.\nlearned great deal scRNA-seq still barely scratching surface. things come across rather new need lot time read .","code":""},{"path":"projects.html","id":"projects","chapter":"6 Projects","heading":"6 Projects","text":"","code":""},{"path":"projects.html","id":"noldus-project","chapter":"6 Projects","heading":"6.1 Noldus project","text":"Noldus Inc. (“Noldus | Advance Behavioral Research” (n.d.)) University Applied sciences Utrecht (website HU) worked team 5 create shiny app help analyse data acquired one research products: Erasmus Ladder. Erasmus Ladder setup consists horizontal ladder determine differences behavior mice recording steps made selection sensors. Variations amount steps made type steps made mice can recorded accurately allows statistical analysis determine differences groups.","code":""},{"path":"projects.html","id":"the-application","chapter":"6 Projects","heading":"6.1.1 The application","text":"application analyses data acquired Erasmus Ladder plots exploratory publication ready graphs together statistical analysis researchers easily interpret results. One challenges creating application figuring kind data interesting researchers. reading articles using device speaking researchers field made selection publication ready graph functions read data formatted Erasmus ladder. also made selection exploratory graphs meant help researches get overview data showing basic information like amount mice used, different groups mice, etc. graphs portray overview things like different types steps mice made different groups.","code":""},{"path":"projects.html","id":"example-graphs","chapter":"6 Projects","heading":"6.1.2 Example graphs","text":"examples data interesting research purposes looked selection different articles. us important criteria articles use Erasmus ladder mostly interested interpreted data . type graphs used portrayed data important us base decisions graphs want application produce based input data. One first graphs found interesting provides clear visual variances steps made different groups mice (María Fernanda Vinueza Veloz et al. 2015). graph two different groups mice, one Purkinje cell deficiency (Pcd) one control group make runs crossing Erasmus ladder. step recorded type steps determined based distance traveled. page 3517 (Fig. 3) article can observe difference variation steps used might caused due Purkinje cell deficiency.\nVersions step type graphs plotted session can also found research (Sathyanesan Gallo 2019; Sathyanesan et al. 2018). graphs depict decrease variation steps used control groups time. might caused mice learning cross bridge efficiently. different type graph looked reaction mice different ques guide mice device. research (M. F. Vinueza Veloz et al. 2012) gave us fresh look data helped guide decisions data might seen relevant.also looked types cerebellum research (Schonewille et al. 2011) selected data better answer research questions. Although mostly applicable project give insight slight differences can .","code":""},{"path":"resume.html","id":"resume","chapter":"7 Resume","heading":"7 Resume","text":"likely already received C.V. case haven’t .","code":""},{"path":"relational-databases.html","id":"relational-databases","chapter":"8 Relational Databases","heading":"8 Relational Databases","text":"Working relational databases means many cases communicate using SQL. pull store data important follows small demonstration can done.first start loading required data short analysis. use data gapminder, data set dslabs package use dengue flu data sets.\nloading data need make tidy make easier work R. also make allot easier work compare data load DBeaver using SQL.finish plot simple graphs show bit data merged.","code":"\n############ Load in Data ##############################################\n## Gapminder data from the dslabs package\n  gapdat <- gapminder\n  gapdata_tidy <- gapminder\n## Dengue en flu data \n  dengue_data <- read_csv(here::here(\"data/dengue_data.csv\"), skip = 10)\n  flu_data <- read_csv(here::here(\"data/flu_data.csv\"), skip = 10)\n########################################################################\n############################## Tidy Data ################################\n\n## Gapminder data looks very tidy so no changes there except for the year column, that needs to become Date\n  gapdata_tidy$year <- as.character(gapdata_tidy$year)\n  colnames(gapdat) <- c(\"country\", \"Date\", \"infant_mortality\", \"life_expectancy\", \"fertility\", \"population\", \"gdp\", \"continent\", \"region\")\n\n## flu data tidy:\n  flu_data_tidy <- flu_data %>% pivot_longer(cols = -c(Date),\n                                             names_to = \"country\",\n                                             values_to = \"count_cases\") \n  ## dengue data tidy:\n  dengue_data_tidy <- dengue_data %>% pivot_longer(cols = -c(Date),\n                                             names_to = \"country\",\n                                             values_to = \"values\") \n########################### Export the Data ############################\n\n## Now we are going to write off the tidy data sets as csv and rds to export them later into the database\n  write_csv(gapdat_tidy, path = here::here(\"data/gapdat_tidy.csv\"))\n  write_csv(flu_data_tidy, path = here::here(\"data/flu_tidy.csv\"))\n  write_csv(dengue_data_tidy, path = here::here(\"data/dengue_tidy.csv\"))\n  \n  write_rds(gapdat_tidy, path = here::here(\"data/gapdat_tidy.rds\"))\n  write_rds(flu_data_tidy, path = here::here(\"data/flu_tidy.rds\"))\n  write_rds(dengue_data_tidy, path = here::here(\"data/dengue_tidy.rds\"))\n\n  ## In DBeaver we now make a new database to store the data\nCREATE DATABASE fludata;## Then in SQL we define the primary keys of the tables (Date and country)\nalter table public.flu_data \n    add constraint PK_flu_data primary key (Date, country);\n    \nalter table public.dengue_data \n    add constraint PK_dengue_data primary key (Date, country);\n\nalter table public.gapminer_data \n    add constraint PK_gapminder_data primary key (Date, country);\n\n## And we inspect the data from the table flu_data to check if all went well\nselect * from flu_data;\n######################## Inspect the Data ######################################\n## Now that the data has been stored in the database we can inspect it in R using the connection made\n\n# Shows the tables\ndbListTables(con)\n\n# Shows the table of gaminder_data\ndbGetQuery(con, 'SELECT * FROM gapminder_data')\n####################### Modify the gapminder data ##############################\n# Next we want to join the 3 data frames together based on the year and country columns\n## flu data tidy:\n  flu_data_tidy <- flu_data_tidy %>% separate(col = Date, into = c(\"year\", \"Month\", \"Day\"), sep = \"-\")\n## dengue data tidy:\n  dengue_data_tidy <- dengue_data_tidy  %>% separate(col = Date, into = c(\"year\", \"Month\", \"Day\"), sep =\"-\")\n    \nsic <- dengue_data_tidy %>% full_join(flu_data_tidy, by=c(\"year\",\"country\", \"Day\", \"Month\"))\n\nmerged_dat <- sic %>% full_join(gapdata_tidy, by=c(\"country\", \"year\"))"},{"path":"parameters.html","id":"parameters","chapter":"9 Parameters","heading":"9 Parameters","text":"page look use parameters Rmarkdown. end use data European Center Disease Control COVID-19 case data. goal create two separate graphs, one showing COVID-19 related cases specified month year country COVID-19 related deaths specified month year country. end make parameters customisable month, year country graphs supposed show.Parameters defined yaml header Rmarkdown documents can used just like variable code. One great advantage can customized fly change function output. case defined three parameters:can change date combinations want country long data input data code work. makes work -though wrote function code custom input function complete Rmarkdown file. handy indeed.page show code can take look parameters used . can find parameters used variables: params$parameter_nameParameters great work save time want change small things code already used . personally prefer writing good functions reusing parameters great tool apply aswell.","code":"\n###################### Load in the Data ########################################\n\n## First lets load in the Data kindly supplied by the ECDC. \n  data <- read_csv(here::here(\"data/covid_data.csv\"))\n\n######################## Data wrangling ########################################\n## Now we are going to change the date data type to date instead of chr.\n  data$dateRep <- as.Date(data$dateRep, \"%d/%m/%Y\")\n\n############################# Graphs ###########################################\n\n## Because the data is already tidy enough we don't need to wrangle further and can simply start plotting the graphs we want.\n  \n# Graph #1, the amount of covid-19 cases in a selected country for a selected period of time\n  data %>% select(month, year, day, cases, countriesAndTerritories, dateRep) %>% \n    filter(dateRep >= as.Date(params$date_from) & dateRep <= as.Date(params$date_until), countriesAndTerritories %in% c(params$country)) %>%\n    ggplot(aes(x=dateRep, y=cases)) + \n    geom_line(aes(color = countriesAndTerritories)) +\n    labs(title= paste(\"Covid-19 Cases in\", params$country),\n         subtitle = paste(params$date_from, params$date_until),\n         x= \"Date\",\n         y= \"Number of Cases\") +\n    scale_color_manual(values=c(\"turquoise3\")) +\n    theme_classic()\n\n# Graph #2, The amount of covid-19 Deaths in a selected country for a selected period of time. If I didn't work with parameters I would have build a function for these two graphs seeing as they are very similar in code. \n  data %>% select(month, year, day, deaths, countriesAndTerritories, dateRep) %>% \n    filter(dateRep >= as.Date(params$date_from) & dateRep <= as.Date(params$date_until), countriesAndTerritories %in% c(params$country)) %>%\n    ggplot(aes(x=dateRep, y=deaths)) + \n    geom_line(aes(color = countriesAndTerritories)) +\n    labs(title= paste(\"Covid-19 Deaths in\", params$country),\n         subtitle = paste(params$date_from, params$date_until),\n         x= \"Date\",\n         y= \"Number of Deaths\") +\n    scale_color_manual(values=\"turquoise4\") +\n    theme_classic()"},{"path":"normalizeme.html","id":"normalizeme","chapter":"10 normalizeme","heading":"10 normalizeme","text":"One skills love improve building usefull packages. don’t much experience enjoy progress making function wich usefull people. first function wrote actually still use min-max scaling normalisation function. end descided create package redefine function , knows maybe someone else might want use !goal normalizeme help normalization data. also contains functions help plot simple graph, case want visualize normalized data quickly insignificant data set just fun .important function package minmax().","code":""},{"path":"normalizeme.html","id":"installation","chapter":"10 normalizeme","heading":"10.1 Installation","text":"can install development version normalizeme GitHub :","code":"\n# install.packages(\"devtools\")\ndevtools::install_github(\"LunarEclipse112/normalizeme\")"},{"path":"references.html","id":"references","chapter":"11 References","heading":"11 References","text":"","code":""},{"path":"references.html","id":"pages","chapter":"11 References","heading":"11.1 404 pages","text":"default, users directed 404 page try access webpage found.","code":""},{"path":"references.html","id":"references-1","chapter":"11 References","heading":"11.2 References","text":"","code":""}]
